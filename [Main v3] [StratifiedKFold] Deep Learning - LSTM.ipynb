{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9576fde0",
   "metadata": {
    "id": "9576fde0"
   },
   "source": [
    "# BREATHING WAVE\n",
    "## DEEP LEARNING - LSTM\n",
    "### 04 March 2023\n",
    "#### V3 = with cross-validation technique included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07cfa282",
   "metadata": {
    "id": "07cfa282"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "# Enable XLA\n",
    "tf.config.optimizer.set_jit(True)\n",
    "\n",
    "df = pd.read_csv(\"breathing_waveform_data.csv\").iloc[:, :-1] # get rid of last column (\"notes\")\n",
    "\n",
    "X = df.iloc[:, :-1]\n",
    "Y = df.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa8bf54a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aa8bf54a",
    "outputId": "f4f08ec0-c57c-4ca4-c131-af36d583eedf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X have a null? \tFalse\n",
      "Y have a null? \tFalse\n"
     ]
    }
   ],
   "source": [
    "# Check if the data do not have any NULL \n",
    "print(\"X have a null? \\t{}\".format(X.isnull().values.any()))\n",
    "print(\"Y have a null? \\t{}\".format(Y.isnull().values.any()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0fa06c9f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "0fa06c9f",
    "outputId": "35e89335-323c-4d86-9678-f74d1acf0b10"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>75</th>\n",
       "      <th>76</th>\n",
       "      <th>77</th>\n",
       "      <th>78</th>\n",
       "      <th>79</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.483309</td>\n",
       "      <td>0.459790</td>\n",
       "      <td>0.431024</td>\n",
       "      <td>0.376565</td>\n",
       "      <td>0.295734</td>\n",
       "      <td>0.193290</td>\n",
       "      <td>0.066060</td>\n",
       "      <td>-0.083445</td>\n",
       "      <td>-0.247221</td>\n",
       "      <td>-0.409374</td>\n",
       "      <td>...</td>\n",
       "      <td>0.332737</td>\n",
       "      <td>0.391514</td>\n",
       "      <td>0.452677</td>\n",
       "      <td>0.521407</td>\n",
       "      <td>0.595845</td>\n",
       "      <td>0.661691</td>\n",
       "      <td>0.702932</td>\n",
       "      <td>0.708613</td>\n",
       "      <td>0.682564</td>\n",
       "      <td>0.637765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-2.044518</td>\n",
       "      <td>-1.935588</td>\n",
       "      <td>-1.808629</td>\n",
       "      <td>-1.667919</td>\n",
       "      <td>-1.513497</td>\n",
       "      <td>-1.348760</td>\n",
       "      <td>-1.171044</td>\n",
       "      <td>-0.972509</td>\n",
       "      <td>-0.759554</td>\n",
       "      <td>-0.547793</td>\n",
       "      <td>...</td>\n",
       "      <td>0.325687</td>\n",
       "      <td>0.138731</td>\n",
       "      <td>-0.053860</td>\n",
       "      <td>-0.241691</td>\n",
       "      <td>-0.417603</td>\n",
       "      <td>-0.582320</td>\n",
       "      <td>-0.738485</td>\n",
       "      <td>-0.889731</td>\n",
       "      <td>-1.037066</td>\n",
       "      <td>-1.174654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.213535</td>\n",
       "      <td>-1.269056</td>\n",
       "      <td>-1.323306</td>\n",
       "      <td>-1.375251</td>\n",
       "      <td>-1.430062</td>\n",
       "      <td>-1.485479</td>\n",
       "      <td>-1.529200</td>\n",
       "      <td>-1.557172</td>\n",
       "      <td>-1.574662</td>\n",
       "      <td>-1.575457</td>\n",
       "      <td>...</td>\n",
       "      <td>0.902226</td>\n",
       "      <td>0.947940</td>\n",
       "      <td>0.996154</td>\n",
       "      <td>1.035743</td>\n",
       "      <td>1.049543</td>\n",
       "      <td>1.024204</td>\n",
       "      <td>0.954716</td>\n",
       "      <td>0.844505</td>\n",
       "      <td>0.702445</td>\n",
       "      <td>0.541555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.914806</td>\n",
       "      <td>-0.887726</td>\n",
       "      <td>-0.856065</td>\n",
       "      <td>-0.823527</td>\n",
       "      <td>-0.794551</td>\n",
       "      <td>-0.768074</td>\n",
       "      <td>-0.740895</td>\n",
       "      <td>-0.713364</td>\n",
       "      <td>-0.685445</td>\n",
       "      <td>-0.652020</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.407344</td>\n",
       "      <td>-0.478218</td>\n",
       "      <td>-0.571465</td>\n",
       "      <td>-0.684115</td>\n",
       "      <td>-0.817078</td>\n",
       "      <td>-0.966231</td>\n",
       "      <td>-1.122537</td>\n",
       "      <td>-1.264759</td>\n",
       "      <td>-1.376908</td>\n",
       "      <td>-1.461059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.547469</td>\n",
       "      <td>-1.458818</td>\n",
       "      <td>-1.362120</td>\n",
       "      <td>-1.264829</td>\n",
       "      <td>-1.164948</td>\n",
       "      <td>-1.060064</td>\n",
       "      <td>-0.954496</td>\n",
       "      <td>-0.849448</td>\n",
       "      <td>-0.742812</td>\n",
       "      <td>-0.636614</td>\n",
       "      <td>...</td>\n",
       "      <td>0.322969</td>\n",
       "      <td>0.227050</td>\n",
       "      <td>0.130983</td>\n",
       "      <td>0.041438</td>\n",
       "      <td>-0.038034</td>\n",
       "      <td>-0.106152</td>\n",
       "      <td>-0.163048</td>\n",
       "      <td>-0.210926</td>\n",
       "      <td>-0.253102</td>\n",
       "      <td>-0.290270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26395</th>\n",
       "      <td>-0.152463</td>\n",
       "      <td>-0.164723</td>\n",
       "      <td>-0.165409</td>\n",
       "      <td>-0.152623</td>\n",
       "      <td>-0.118115</td>\n",
       "      <td>-0.066218</td>\n",
       "      <td>-0.010253</td>\n",
       "      <td>0.041637</td>\n",
       "      <td>0.092217</td>\n",
       "      <td>0.140510</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.345803</td>\n",
       "      <td>-0.336787</td>\n",
       "      <td>-0.306774</td>\n",
       "      <td>-0.280607</td>\n",
       "      <td>-0.269843</td>\n",
       "      <td>-0.260062</td>\n",
       "      <td>-0.229981</td>\n",
       "      <td>-0.167654</td>\n",
       "      <td>-0.082300</td>\n",
       "      <td>0.004372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26396</th>\n",
       "      <td>-0.164723</td>\n",
       "      <td>-0.165409</td>\n",
       "      <td>-0.152623</td>\n",
       "      <td>-0.118115</td>\n",
       "      <td>-0.066218</td>\n",
       "      <td>-0.010253</td>\n",
       "      <td>0.041637</td>\n",
       "      <td>0.092217</td>\n",
       "      <td>0.140510</td>\n",
       "      <td>0.188025</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.336787</td>\n",
       "      <td>-0.306774</td>\n",
       "      <td>-0.280607</td>\n",
       "      <td>-0.269843</td>\n",
       "      <td>-0.260062</td>\n",
       "      <td>-0.229981</td>\n",
       "      <td>-0.167654</td>\n",
       "      <td>-0.082300</td>\n",
       "      <td>0.004372</td>\n",
       "      <td>0.089958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26397</th>\n",
       "      <td>-0.165409</td>\n",
       "      <td>-0.152623</td>\n",
       "      <td>-0.118115</td>\n",
       "      <td>-0.066218</td>\n",
       "      <td>-0.010253</td>\n",
       "      <td>0.041637</td>\n",
       "      <td>0.092217</td>\n",
       "      <td>0.140510</td>\n",
       "      <td>0.188025</td>\n",
       "      <td>0.240939</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.306774</td>\n",
       "      <td>-0.280607</td>\n",
       "      <td>-0.269843</td>\n",
       "      <td>-0.260062</td>\n",
       "      <td>-0.229981</td>\n",
       "      <td>-0.167654</td>\n",
       "      <td>-0.082300</td>\n",
       "      <td>0.004372</td>\n",
       "      <td>0.089958</td>\n",
       "      <td>0.179209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26398</th>\n",
       "      <td>-0.152623</td>\n",
       "      <td>-0.118115</td>\n",
       "      <td>-0.066218</td>\n",
       "      <td>-0.010253</td>\n",
       "      <td>0.041637</td>\n",
       "      <td>0.092217</td>\n",
       "      <td>0.140510</td>\n",
       "      <td>0.188025</td>\n",
       "      <td>0.240939</td>\n",
       "      <td>0.294399</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.280607</td>\n",
       "      <td>-0.269843</td>\n",
       "      <td>-0.260062</td>\n",
       "      <td>-0.229981</td>\n",
       "      <td>-0.167654</td>\n",
       "      <td>-0.082300</td>\n",
       "      <td>0.004372</td>\n",
       "      <td>0.089958</td>\n",
       "      <td>0.179209</td>\n",
       "      <td>0.264014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26399</th>\n",
       "      <td>-0.118115</td>\n",
       "      <td>-0.066218</td>\n",
       "      <td>-0.010253</td>\n",
       "      <td>0.041637</td>\n",
       "      <td>0.092217</td>\n",
       "      <td>0.140510</td>\n",
       "      <td>0.188025</td>\n",
       "      <td>0.240939</td>\n",
       "      <td>0.294399</td>\n",
       "      <td>0.340346</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.269843</td>\n",
       "      <td>-0.260062</td>\n",
       "      <td>-0.229981</td>\n",
       "      <td>-0.167654</td>\n",
       "      <td>-0.082300</td>\n",
       "      <td>0.004372</td>\n",
       "      <td>0.089958</td>\n",
       "      <td>0.179209</td>\n",
       "      <td>0.264014</td>\n",
       "      <td>0.343418</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26400 rows × 85 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1         2         3         4         5         6  \\\n",
       "0      0.483309  0.459790  0.431024  0.376565  0.295734  0.193290  0.066060   \n",
       "1     -2.044518 -1.935588 -1.808629 -1.667919 -1.513497 -1.348760 -1.171044   \n",
       "2     -1.213535 -1.269056 -1.323306 -1.375251 -1.430062 -1.485479 -1.529200   \n",
       "3     -0.914806 -0.887726 -0.856065 -0.823527 -0.794551 -0.768074 -0.740895   \n",
       "4     -1.547469 -1.458818 -1.362120 -1.264829 -1.164948 -1.060064 -0.954496   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "26395 -0.152463 -0.164723 -0.165409 -0.152623 -0.118115 -0.066218 -0.010253   \n",
       "26396 -0.164723 -0.165409 -0.152623 -0.118115 -0.066218 -0.010253  0.041637   \n",
       "26397 -0.165409 -0.152623 -0.118115 -0.066218 -0.010253  0.041637  0.092217   \n",
       "26398 -0.152623 -0.118115 -0.066218 -0.010253  0.041637  0.092217  0.140510   \n",
       "26399 -0.118115 -0.066218 -0.010253  0.041637  0.092217  0.140510  0.188025   \n",
       "\n",
       "              7         8         9  ...        75        76        77  \\\n",
       "0     -0.083445 -0.247221 -0.409374  ...  0.332737  0.391514  0.452677   \n",
       "1     -0.972509 -0.759554 -0.547793  ...  0.325687  0.138731 -0.053860   \n",
       "2     -1.557172 -1.574662 -1.575457  ...  0.902226  0.947940  0.996154   \n",
       "3     -0.713364 -0.685445 -0.652020  ... -0.407344 -0.478218 -0.571465   \n",
       "4     -0.849448 -0.742812 -0.636614  ...  0.322969  0.227050  0.130983   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "26395  0.041637  0.092217  0.140510  ... -0.345803 -0.336787 -0.306774   \n",
       "26396  0.092217  0.140510  0.188025  ... -0.336787 -0.306774 -0.280607   \n",
       "26397  0.140510  0.188025  0.240939  ... -0.306774 -0.280607 -0.269843   \n",
       "26398  0.188025  0.240939  0.294399  ... -0.280607 -0.269843 -0.260062   \n",
       "26399  0.240939  0.294399  0.340346  ... -0.269843 -0.260062 -0.229981   \n",
       "\n",
       "             78        79        80        81        82        83        84  \n",
       "0      0.521407  0.595845  0.661691  0.702932  0.708613  0.682564  0.637765  \n",
       "1     -0.241691 -0.417603 -0.582320 -0.738485 -0.889731 -1.037066 -1.174654  \n",
       "2      1.035743  1.049543  1.024204  0.954716  0.844505  0.702445  0.541555  \n",
       "3     -0.684115 -0.817078 -0.966231 -1.122537 -1.264759 -1.376908 -1.461059  \n",
       "4      0.041438 -0.038034 -0.106152 -0.163048 -0.210926 -0.253102 -0.290270  \n",
       "...         ...       ...       ...       ...       ...       ...       ...  \n",
       "26395 -0.280607 -0.269843 -0.260062 -0.229981 -0.167654 -0.082300  0.004372  \n",
       "26396 -0.269843 -0.260062 -0.229981 -0.167654 -0.082300  0.004372  0.089958  \n",
       "26397 -0.260062 -0.229981 -0.167654 -0.082300  0.004372  0.089958  0.179209  \n",
       "26398 -0.229981 -0.167654 -0.082300  0.004372  0.089958  0.179209  0.264014  \n",
       "26399 -0.167654 -0.082300  0.004372  0.089958  0.179209  0.264014  0.343418  \n",
       "\n",
       "[26400 rows x 85 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1b3592e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b1b3592e",
    "outputId": "14191ef1-d3ce-4982-83b9-ecf9bcf5312d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "normal        19734\n",
       "quick          2667\n",
       "hold           2133\n",
       "deep           1066\n",
       "deep_quick      800\n",
       "Name: labels, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2b0906",
   "metadata": {
    "id": "4c2b0906"
   },
   "source": [
    "### Program Starting\n",
    "# PART 1 : Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0723f193",
   "metadata": {
    "id": "0723f193"
   },
   "source": [
    "## Hot Encoded The Label Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0322a049",
   "metadata": {
    "id": "0322a049"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# encode class values as integers [0,0,0,0,0,0,0,1,1,1,1,1,2,2,2,2]\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(Y)\n",
    "encoded_Y = encoder.transform(Y)\n",
    "\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "hot_y = np_utils.to_categorical(encoded_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5279137d",
   "metadata": {
    "id": "5279137d"
   },
   "source": [
    "## Scale The Training Data (STD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0513ed4",
   "metadata": {
    "id": "b0513ed4"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X = sc.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1be3b8",
   "metadata": {
    "id": "9b1be3b8"
   },
   "source": [
    "## Reshaping The Training Data to 3-Dimensional Numpy Array\n",
    "### STRUCTURE : (batch_size, timestep, feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0456d564",
   "metadata": {
    "id": "0456d564"
   },
   "outputs": [],
   "source": [
    "timestep = 5\n",
    "X = np.reshape(X, (X.shape[0], int(85/timestep), timestep))\n",
    "# (26400, 17, 5)\n",
    "# 5 indicator will be used per sequence/timestep per sample/row"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbc0ca7",
   "metadata": {
    "id": "bcbc0ca7"
   },
   "source": [
    "# PART 2 : Building The RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d100f8a8",
   "metadata": {
    "id": "d100f8a8"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oFcb5tIpis9h",
   "metadata": {
    "id": "oFcb5tIpis9h"
   },
   "source": [
    "## Creating Layer of RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc7f5437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for Model Structure\n",
    "from keras.optimizers import Adam\n",
    "_optimizer = Adam()\n",
    "_loss = \"categorical_crossentropy\"\n",
    "_metric = [\"accuracy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bfbb09d5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bfbb09d5",
    "outputId": "10305db0-f336-4336-d54f-4a100ebc1152"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 17, 60)            15840     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 17, 60)            0         \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 17, 60)            29040     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 17, 60)            0         \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 60)                29040     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 60)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 5)                 305       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 74,225\n",
      "Trainable params: 74,225\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "classifier = Sequential()\n",
    "\n",
    "# first layer\n",
    "classifier.add(LSTM(units=60, return_sequences=True, input_shape=(17, 5)))\n",
    "classifier.add(Dropout(0.2))    # Ignore 20% of the neuron (ex. 50 * 20% = 10 neuoron will be ignored) \n",
    "\n",
    "# second layer\n",
    "classifier.add(LSTM(units=60, return_sequences=True))\n",
    "classifier.add(Dropout(0.2))\n",
    "\n",
    "# third layer\n",
    "# classifier.add(LSTM(units=20, return_sequences=True))\n",
    "# classifier.add(Dropout(0.2))\n",
    "\n",
    "# fourth layer\n",
    "classifier.add(LSTM(units=60))\n",
    "classifier.add(Dropout(0.2))\n",
    "\n",
    "# last layer\n",
    "classifier.add(Dense(units=5, activation='softmax'))\n",
    "\n",
    "# Compile\n",
    "classifier.compile(optimizer=_optimizer, loss=_loss, metrics=_metric)\n",
    "\n",
    "# Plot Summary of Model\n",
    "classifier.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gYHxGBbTjiOO",
   "metadata": {
    "id": "gYHxGBbTjiOO"
   },
   "source": [
    "# PART 3 : Training Time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ngysebSjIBl",
   "metadata": {
    "id": "3ngysebSjIBl"
   },
   "source": [
    "## Train the Model - Cross Validation (Stratified K-Fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7wCqc9xqG8l7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7wCqc9xqG8l7",
    "outputId": "db6e0034-41fa-4a02-a6d8-ae353c0b1a97"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1\n",
      "Epoch 1/15\n",
      "743/743 [==============================] - 14s 14ms/step - loss: 0.7182 - accuracy: 0.7670 - val_loss: 0.6316 - val_accuracy: 0.7731\n",
      "Epoch 2/15\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.5726 - accuracy: 0.7849 - val_loss: 0.4692 - val_accuracy: 0.8220\n",
      "Epoch 3/15\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.4298 - accuracy: 0.8370 - val_loss: 0.3490 - val_accuracy: 0.8777\n",
      "Epoch 4/15\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.3235 - accuracy: 0.8847 - val_loss: 0.2574 - val_accuracy: 0.9121\n",
      "Epoch 5/15\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.2485 - accuracy: 0.9162 - val_loss: 0.1914 - val_accuracy: 0.9413\n",
      "Epoch 6/15\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.1952 - accuracy: 0.9366 - val_loss: 0.1532 - val_accuracy: 0.9511\n",
      "Epoch 7/15\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.1537 - accuracy: 0.9494 - val_loss: 0.1078 - val_accuracy: 0.9689\n",
      "Epoch 8/15\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.1283 - accuracy: 0.9588 - val_loss: 0.1253 - val_accuracy: 0.9617\n",
      "Epoch 9/15\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.1043 - accuracy: 0.9678 - val_loss: 0.0970 - val_accuracy: 0.9727\n",
      "Epoch 10/15\n",
      "743/743 [==============================] - 10s 14ms/step - loss: 0.0906 - accuracy: 0.9725 - val_loss: 0.0734 - val_accuracy: 0.9811\n",
      "Epoch 11/15\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.0830 - accuracy: 0.9755 - val_loss: 0.0744 - val_accuracy: 0.9811\n",
      "Epoch 12/15\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.0767 - accuracy: 0.9782 - val_loss: 0.0621 - val_accuracy: 0.9848\n",
      "Epoch 13/15\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.0703 - accuracy: 0.9789 - val_loss: 0.0711 - val_accuracy: 0.9792\n",
      "Epoch 14/15\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.0707 - accuracy: 0.9798 - val_loss: 0.0577 - val_accuracy: 0.9856\n",
      "Epoch 15/15\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.0660 - accuracy: 0.9816 - val_loss: 0.0598 - val_accuracy: 0.9822\n",
      "Fold: 2\n",
      "Epoch 1/15\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.7485 - accuracy: 0.7618 - val_loss: 0.6718 - val_accuracy: 0.7682\n",
      "Epoch 2/15\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.6624 - accuracy: 0.7699 - val_loss: 0.6265 - val_accuracy: 0.7716\n",
      "Epoch 3/15\n",
      "743/743 [==============================] - 10s 14ms/step - loss: 0.6138 - accuracy: 0.7747 - val_loss: 0.5897 - val_accuracy: 0.7750\n",
      "Epoch 4/15\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.5521 - accuracy: 0.7872 - val_loss: 0.4678 - val_accuracy: 0.8246\n",
      "Epoch 5/15\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.4343 - accuracy: 0.8350 - val_loss: 0.3429 - val_accuracy: 0.8742\n",
      "Epoch 6/15\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.3278 - accuracy: 0.8828 - val_loss: 0.2613 - val_accuracy: 0.9102\n",
      "Epoch 7/15\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.2576 - accuracy: 0.9138 - val_loss: 0.2138 - val_accuracy: 0.9341\n",
      "Epoch 8/15\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.1990 - accuracy: 0.9348 - val_loss: 0.1356 - val_accuracy: 0.9633\n",
      "Epoch 9/15\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.1606 - accuracy: 0.9495 - val_loss: 0.1351 - val_accuracy: 0.9561\n",
      "Epoch 10/15\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.1329 - accuracy: 0.9580 - val_loss: 0.0998 - val_accuracy: 0.9701\n",
      "Epoch 11/15\n",
      "743/743 [==============================] - 10s 14ms/step - loss: 0.1080 - accuracy: 0.9676 - val_loss: 0.0746 - val_accuracy: 0.9784\n",
      "Epoch 12/15\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.1042 - accuracy: 0.9684 - val_loss: 0.0883 - val_accuracy: 0.9735\n",
      "Epoch 13/15\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.0937 - accuracy: 0.9722 - val_loss: 0.0670 - val_accuracy: 0.9811\n",
      "Epoch 14/15\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.0849 - accuracy: 0.9746 - val_loss: 0.0835 - val_accuracy: 0.9723\n",
      "Epoch 15/15\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.0783 - accuracy: 0.9765 - val_loss: 0.0564 - val_accuracy: 0.9841\n",
      "Fold: 3\n",
      "Epoch 1/15\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.7506 - accuracy: 0.7626 - val_loss: 0.6675 - val_accuracy: 0.7693\n",
      "Epoch 2/15\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.6590 - accuracy: 0.7706 - val_loss: 0.6346 - val_accuracy: 0.7723\n",
      "Epoch 3/15\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.6151 - accuracy: 0.7744 - val_loss: 0.5858 - val_accuracy: 0.7731\n",
      "Epoch 4/15\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.5499 - accuracy: 0.7887 - val_loss: 0.4990 - val_accuracy: 0.8042\n",
      "Epoch 5/15\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.4531 - accuracy: 0.8319 - val_loss: 0.3959 - val_accuracy: 0.8614\n",
      "Epoch 6/15\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.3384 - accuracy: 0.8782 - val_loss: 0.2841 - val_accuracy: 0.8958\n",
      "Epoch 7/15\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.2507 - accuracy: 0.9162 - val_loss: 0.2143 - val_accuracy: 0.9261\n",
      "Epoch 8/15\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.1848 - accuracy: 0.9399 - val_loss: 0.1704 - val_accuracy: 0.9477\n",
      "Epoch 9/15\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.1575 - accuracy: 0.9511 - val_loss: 0.1530 - val_accuracy: 0.9538\n",
      "Epoch 10/15\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.1275 - accuracy: 0.9612 - val_loss: 0.1230 - val_accuracy: 0.9629\n",
      "Epoch 11/15\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.1147 - accuracy: 0.9650 - val_loss: 0.0962 - val_accuracy: 0.9674\n",
      "Epoch 12/15\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.0965 - accuracy: 0.9721 - val_loss: 0.0874 - val_accuracy: 0.9780\n",
      "Epoch 13/15\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.0882 - accuracy: 0.9744 - val_loss: 0.0932 - val_accuracy: 0.9742\n",
      "Epoch 14/15\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.0848 - accuracy: 0.9756 - val_loss: 0.0744 - val_accuracy: 0.9792\n",
      "Epoch 15/15\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.0750 - accuracy: 0.9788 - val_loss: 0.0765 - val_accuracy: 0.9792\n",
      "Fold: 4\n",
      "Epoch 1/15\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.7410 - accuracy: 0.7636 - val_loss: 0.6666 - val_accuracy: 0.7689\n",
      "Epoch 2/15\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.6552 - accuracy: 0.7708 - val_loss: 0.6299 - val_accuracy: 0.7670\n",
      "Epoch 3/15\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.5992 - accuracy: 0.7745 - val_loss: 0.5738 - val_accuracy: 0.7758\n",
      "Epoch 4/15\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.5154 - accuracy: 0.7994 - val_loss: 0.4661 - val_accuracy: 0.8159\n",
      "Epoch 5/15\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.4153 - accuracy: 0.8444 - val_loss: 0.3271 - val_accuracy: 0.8837\n",
      "Epoch 6/15\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.3004 - accuracy: 0.8970 - val_loss: 0.2543 - val_accuracy: 0.9152\n",
      "Epoch 7/15\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.2240 - accuracy: 0.9259 - val_loss: 0.1811 - val_accuracy: 0.9383\n",
      "Epoch 8/15\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.1800 - accuracy: 0.9434 - val_loss: 0.1431 - val_accuracy: 0.9561\n",
      "Epoch 9/15\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.1468 - accuracy: 0.9542 - val_loss: 0.1542 - val_accuracy: 0.9489\n",
      "Epoch 10/15\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.1295 - accuracy: 0.9599 - val_loss: 0.1045 - val_accuracy: 0.9686\n",
      "Epoch 11/15\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.1097 - accuracy: 0.9660 - val_loss: 0.0928 - val_accuracy: 0.9746\n",
      "Epoch 12/15\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.0942 - accuracy: 0.9710 - val_loss: 0.0906 - val_accuracy: 0.9750\n",
      "Epoch 13/15\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.0878 - accuracy: 0.9740 - val_loss: 0.0770 - val_accuracy: 0.9761\n",
      "Epoch 14/15\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.0841 - accuracy: 0.9748 - val_loss: 0.0794 - val_accuracy: 0.9773\n",
      "Epoch 15/15\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.0744 - accuracy: 0.9777 - val_loss: 0.0724 - val_accuracy: 0.9792\n",
      "Fold: 5\n",
      "Epoch 1/15\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.7486 - accuracy: 0.7624 - val_loss: 0.6818 - val_accuracy: 0.7674\n",
      "Epoch 2/15\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.6593 - accuracy: 0.7707 - val_loss: 0.6615 - val_accuracy: 0.7727\n",
      "Epoch 3/15\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.6079 - accuracy: 0.7728 - val_loss: 0.5935 - val_accuracy: 0.7761\n",
      "Epoch 4/15\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.5550 - accuracy: 0.7831 - val_loss: 0.4874 - val_accuracy: 0.8098\n",
      "Epoch 5/15\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.4481 - accuracy: 0.8302 - val_loss: 0.4287 - val_accuracy: 0.8580\n",
      "Epoch 6/15\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.3343 - accuracy: 0.8790 - val_loss: 0.2836 - val_accuracy: 0.8928\n",
      "Epoch 7/15\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.2518 - accuracy: 0.9140 - val_loss: 0.2265 - val_accuracy: 0.9186\n",
      "Epoch 8/15\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.1943 - accuracy: 0.9391 - val_loss: 0.1392 - val_accuracy: 0.9557\n",
      "Epoch 9/15\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.1585 - accuracy: 0.9505 - val_loss: 0.1319 - val_accuracy: 0.9614\n",
      "Epoch 10/15\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.1319 - accuracy: 0.9595 - val_loss: 0.0934 - val_accuracy: 0.9742\n",
      "Epoch 11/15\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.1055 - accuracy: 0.9695 - val_loss: 0.0778 - val_accuracy: 0.9773\n",
      "Epoch 12/15\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.0989 - accuracy: 0.9703 - val_loss: 0.0660 - val_accuracy: 0.9803\n",
      "Epoch 13/15\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.0872 - accuracy: 0.9741 - val_loss: 0.0806 - val_accuracy: 0.9746\n",
      "Epoch 14/15\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.0823 - accuracy: 0.9747 - val_loss: 0.0644 - val_accuracy: 0.9784\n",
      "Epoch 15/15\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.0740 - accuracy: 0.9780 - val_loss: 0.0533 - val_accuracy: 0.9845\n",
      "Fold: 6\n",
      "Epoch 1/15\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.7466 - accuracy: 0.7625 - val_loss: 0.6641 - val_accuracy: 0.7708\n",
      "Epoch 2/15\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.6554 - accuracy: 0.7699 - val_loss: 0.6532 - val_accuracy: 0.7697\n",
      "Epoch 3/15\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.6020 - accuracy: 0.7786 - val_loss: 0.5602 - val_accuracy: 0.7917\n",
      "Epoch 4/15\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.5319 - accuracy: 0.7987 - val_loss: 0.4652 - val_accuracy: 0.8307\n",
      "Epoch 5/15\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.4252 - accuracy: 0.8425 - val_loss: 0.3435 - val_accuracy: 0.8758\n",
      "Epoch 6/15\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.3159 - accuracy: 0.8878 - val_loss: 0.2600 - val_accuracy: 0.9152\n",
      "Epoch 7/15\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.2360 - accuracy: 0.9204 - val_loss: 0.1823 - val_accuracy: 0.9420\n",
      "Epoch 8/15\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.1806 - accuracy: 0.9416 - val_loss: 0.1379 - val_accuracy: 0.9549\n",
      "Epoch 9/15\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.1410 - accuracy: 0.9550 - val_loss: 0.1215 - val_accuracy: 0.9614\n",
      "Epoch 10/15\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.1227 - accuracy: 0.9614 - val_loss: 0.1178 - val_accuracy: 0.9606\n",
      "Epoch 11/15\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.1054 - accuracy: 0.9673 - val_loss: 0.0839 - val_accuracy: 0.9765\n",
      "Epoch 12/15\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.0973 - accuracy: 0.9697 - val_loss: 0.0777 - val_accuracy: 0.9777\n",
      "Epoch 13/15\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.0841 - accuracy: 0.9740 - val_loss: 0.0757 - val_accuracy: 0.9811\n",
      "Epoch 14/15\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.0746 - accuracy: 0.9770 - val_loss: 0.0715 - val_accuracy: 0.9780\n",
      "Epoch 15/15\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.0757 - accuracy: 0.9773 - val_loss: 0.0593 - val_accuracy: 0.9845\n",
      "Fold: 7\n",
      "Epoch 1/15\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.7494 - accuracy: 0.7616 - val_loss: 0.6913 - val_accuracy: 0.7610\n",
      "Epoch 2/15\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.6599 - accuracy: 0.7706 - val_loss: 0.6262 - val_accuracy: 0.7769\n",
      "Epoch 3/15\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.6140 - accuracy: 0.7743 - val_loss: 0.5837 - val_accuracy: 0.7845\n",
      "Epoch 4/15\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.5640 - accuracy: 0.7831 - val_loss: 0.5196 - val_accuracy: 0.8061\n",
      "Epoch 5/15\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.4542 - accuracy: 0.8282 - val_loss: 0.4210 - val_accuracy: 0.8371\n",
      "Epoch 6/15\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.3580 - accuracy: 0.8765 - val_loss: 0.2806 - val_accuracy: 0.9083\n",
      "Epoch 7/15\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.2496 - accuracy: 0.9186 - val_loss: 0.1889 - val_accuracy: 0.9420\n",
      "Epoch 8/15\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.1880 - accuracy: 0.9382 - val_loss: 0.1377 - val_accuracy: 0.9583\n",
      "Epoch 9/15\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.1466 - accuracy: 0.9531 - val_loss: 0.1161 - val_accuracy: 0.9682\n",
      "Epoch 10/15\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.1287 - accuracy: 0.9602 - val_loss: 0.0971 - val_accuracy: 0.9708\n",
      "Epoch 11/15\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.1102 - accuracy: 0.9658 - val_loss: 0.0917 - val_accuracy: 0.9731\n",
      "Epoch 12/15\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.1049 - accuracy: 0.9677 - val_loss: 0.0796 - val_accuracy: 0.9777\n",
      "Epoch 13/15\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.0910 - accuracy: 0.9729 - val_loss: 0.0812 - val_accuracy: 0.9723\n",
      "Epoch 14/15\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.0861 - accuracy: 0.9750 - val_loss: 0.0687 - val_accuracy: 0.9811\n",
      "Epoch 15/15\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.0863 - accuracy: 0.9742 - val_loss: 0.0639 - val_accuracy: 0.9803\n",
      "Fold: 8\n",
      "Epoch 1/15\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.7494 - accuracy: 0.7613 - val_loss: 0.6702 - val_accuracy: 0.7708\n",
      "Epoch 2/15\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.6588 - accuracy: 0.7701 - val_loss: 0.6120 - val_accuracy: 0.7761\n",
      "Epoch 3/15\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.6059 - accuracy: 0.7765 - val_loss: 0.5753 - val_accuracy: 0.7886\n",
      "Epoch 4/15\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.5372 - accuracy: 0.7989 - val_loss: 0.4831 - val_accuracy: 0.8208\n",
      "Epoch 5/15\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.4513 - accuracy: 0.8367 - val_loss: 0.3423 - val_accuracy: 0.8754\n",
      "Epoch 6/15\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.3315 - accuracy: 0.8854 - val_loss: 0.2882 - val_accuracy: 0.9004\n",
      "Epoch 7/15\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.2467 - accuracy: 0.9143 - val_loss: 0.1935 - val_accuracy: 0.9360\n",
      "Epoch 8/15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "743/743 [==============================] - 10s 13ms/step - loss: 0.1950 - accuracy: 0.9361 - val_loss: 0.1426 - val_accuracy: 0.9568\n",
      "Epoch 9/15\n",
      "743/743 [==============================] - 11s 15ms/step - loss: 0.1540 - accuracy: 0.9503 - val_loss: 0.1069 - val_accuracy: 0.9655\n",
      "Epoch 10/15\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.1324 - accuracy: 0.9581 - val_loss: 0.1126 - val_accuracy: 0.9644\n",
      "Epoch 11/15\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.1155 - accuracy: 0.9637 - val_loss: 0.1020 - val_accuracy: 0.9674\n",
      "Epoch 12/15\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.0968 - accuracy: 0.9695 - val_loss: 0.0761 - val_accuracy: 0.9750\n",
      "Epoch 13/15\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.0906 - accuracy: 0.9710 - val_loss: 0.0813 - val_accuracy: 0.9720\n",
      "Epoch 14/15\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.0841 - accuracy: 0.9751 - val_loss: 0.0657 - val_accuracy: 0.9807\n",
      "Epoch 15/15\n",
      "743/743 [==============================] - 13s 17ms/step - loss: 0.0733 - accuracy: 0.9787 - val_loss: 0.0590 - val_accuracy: 0.9799\n",
      "Fold: 9\n",
      "Epoch 1/15\n",
      "743/743 [==============================] - 10s 14ms/step - loss: 0.7481 - accuracy: 0.7615 - val_loss: 0.6721 - val_accuracy: 0.7727\n",
      "Epoch 2/15\n",
      "743/743 [==============================] - 13s 17ms/step - loss: 0.6506 - accuracy: 0.7709 - val_loss: 0.6000 - val_accuracy: 0.7780\n",
      "Epoch 3/15\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.6063 - accuracy: 0.7735 - val_loss: 0.5573 - val_accuracy: 0.7898\n",
      "Epoch 4/15\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.5315 - accuracy: 0.7999 - val_loss: 0.4851 - val_accuracy: 0.8174\n",
      "Epoch 5/15\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.4366 - accuracy: 0.8400 - val_loss: 0.3450 - val_accuracy: 0.8742\n",
      "Epoch 6/15\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.3129 - accuracy: 0.8924 - val_loss: 0.2127 - val_accuracy: 0.9314\n",
      "Epoch 7/15\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.2171 - accuracy: 0.9290 - val_loss: 0.1542 - val_accuracy: 0.9538\n",
      "Epoch 8/15\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.1685 - accuracy: 0.9467 - val_loss: 0.1375 - val_accuracy: 0.9523\n",
      "Epoch 9/15\n",
      "743/743 [==============================] - 11s 14ms/step - loss: 0.1363 - accuracy: 0.9574 - val_loss: 0.1089 - val_accuracy: 0.9667\n",
      "Epoch 10/15\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.1224 - accuracy: 0.9604 - val_loss: 0.0869 - val_accuracy: 0.9742\n",
      "Epoch 11/15\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.1045 - accuracy: 0.9679 - val_loss: 0.0776 - val_accuracy: 0.9769\n",
      "Epoch 12/15\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.0955 - accuracy: 0.9705 - val_loss: 0.0626 - val_accuracy: 0.9852\n",
      "Epoch 13/15\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.0888 - accuracy: 0.9734 - val_loss: 0.0702 - val_accuracy: 0.9799\n",
      "Epoch 14/15\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.0731 - accuracy: 0.9783 - val_loss: 0.0806 - val_accuracy: 0.9795\n",
      "Epoch 15/15\n",
      "743/743 [==============================] - 1235s 2s/step - loss: 0.0773 - accuracy: 0.9764 - val_loss: 0.0779 - val_accuracy: 0.9761\n",
      "Fold: 10\n",
      "Epoch 1/15\n",
      "743/743 [==============================] - 10s 14ms/step - loss: 0.7531 - accuracy: 0.7606 - val_loss: 0.6603 - val_accuracy: 0.7693\n",
      "Epoch 2/15\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.6619 - accuracy: 0.7699 - val_loss: 0.6089 - val_accuracy: 0.7773\n",
      "Epoch 3/15\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.6001 - accuracy: 0.7763 - val_loss: 0.5579 - val_accuracy: 0.7936\n",
      "Epoch 4/15\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.5142 - accuracy: 0.8057 - val_loss: 0.4536 - val_accuracy: 0.8250\n",
      "Epoch 5/15\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.4164 - accuracy: 0.8498 - val_loss: 0.3466 - val_accuracy: 0.8807\n",
      "Epoch 6/15\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.3046 - accuracy: 0.8978 - val_loss: 0.2606 - val_accuracy: 0.9148\n",
      "Epoch 7/15\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.2304 - accuracy: 0.9263 - val_loss: 0.2061 - val_accuracy: 0.9356\n",
      "Epoch 8/15\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.1789 - accuracy: 0.9431 - val_loss: 0.1543 - val_accuracy: 0.9549\n",
      "Epoch 9/15\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.1452 - accuracy: 0.9548 - val_loss: 0.1327 - val_accuracy: 0.9617\n",
      "Epoch 10/15\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.1302 - accuracy: 0.9595 - val_loss: 0.1441 - val_accuracy: 0.9557\n",
      "Epoch 11/15\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.1129 - accuracy: 0.9654 - val_loss: 0.1065 - val_accuracy: 0.9678\n",
      "Epoch 12/15\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.0983 - accuracy: 0.9708 - val_loss: 0.1008 - val_accuracy: 0.9682\n",
      "Epoch 13/15\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.0924 - accuracy: 0.9726 - val_loss: 0.0810 - val_accuracy: 0.9739\n",
      "Epoch 14/15\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.0832 - accuracy: 0.9740 - val_loss: 0.0848 - val_accuracy: 0.9739\n",
      "Epoch 15/15\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.0839 - accuracy: 0.9745 - val_loss: 0.0839 - val_accuracy: 0.9742\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.initializers import GlorotUniform\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Setting for training arguments (epoch, batch_size)\n",
    "ep = 15       # epoch\n",
    "bt = 32        # batch_size\n",
    "\n",
    "# Create an instance of StratifiedKFold with the desired number of folds\n",
    "n_splits = 10\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=21)\n",
    "\n",
    "# encode class values as integers [0,0,0,0,0,0,0,1,1,1,1,1,2,2,2,2]\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "history = {'loss': [], 'val_loss': [], 'accuracy': [], 'val_accuracy': []}\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X, Y)):\n",
    "    print('Fold:', fold+1)\n",
    "    X_train, Y_train = X[train_idx], hot_y[train_idx]\n",
    "    X_val, Y_val = X[val_idx], hot_y[val_idx]\n",
    "    \n",
    "\n",
    "    # Train the model on the training set\n",
    "    classifier.fit(X_train, Y_train, epochs=ep, batch_size=bt, validation_data=(X_val, Y_val))\n",
    "    # Record the evaluation metric for this fold - training data\n",
    "    history['loss'].append(classifier.history.history['loss'][-1] * 100)\n",
    "    history['accuracy'].append(classifier.history.history['accuracy'][-1] * 100)\n",
    "    \n",
    "    # Evaluate the model on the validation set\n",
    "    loss, accuracy = classifier.evaluate(X_val, Y_val, verbose=0)\n",
    "    # print('Validation Loss\\t\\t: {:.2f}%'.format(loss*100))\n",
    "    # print('Validation Accuracy\\t: {:.2f}%'.format(accuracy*100))\n",
    "\n",
    "    # Record the evaluation metric for this fold - validation data\n",
    "    history['val_loss'].append(loss*100)\n",
    "    history['val_accuracy'].append(accuracy*100)\n",
    "    \n",
    "    # saving the model\n",
    "    filename = \"{}\\\\{}\\\\{}-f{}.h5\".format(os.getcwd(), \"MODELS\\\\[3-layer] - 3L1\\\\CV\\\\StratifiedKFold\", fold+1, n_splits)\n",
    "    classifier.save(filename)\n",
    "    \n",
    "    # clear all the weight - set all weight to random number\n",
    "    classifier.set_weights([GlorotUniform(seed=21)(w.shape) for w in classifier.weights])\n",
    "\n",
    "# Calculate the mean and standard deviation of the evaluation metric across all folds\n",
    "mean_loss = np.mean(history['val_loss'], axis=0)\n",
    "std_loss = np.std(history['val_loss'], axis=0)\n",
    "mean_accuracy = np.mean(history['val_accuracy'], axis=0)\n",
    "std_accuracy = np.std(history['val_accuracy'], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "724a6697",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [6.596960872411728,\n",
       "  7.834737002849579,\n",
       "  7.499384880065918,\n",
       "  7.442329823970795,\n",
       "  7.403828203678131,\n",
       "  7.570268213748932,\n",
       "  8.628030121326447,\n",
       "  7.327746599912643,\n",
       "  7.730916142463684,\n",
       "  8.394622057676315],\n",
       " 'val_loss': [5.978681519627571,\n",
       "  5.635573714971542,\n",
       "  7.653045654296875,\n",
       "  7.240819931030273,\n",
       "  5.328895151615143,\n",
       "  5.9324003756046295,\n",
       "  6.3884928822517395,\n",
       "  5.897023901343346,\n",
       "  7.794691622257233,\n",
       "  8.385276794433594],\n",
       " 'accuracy': [98.1607735157013,\n",
       "  97.64730930328369,\n",
       "  97.8787899017334,\n",
       "  97.76515364646912,\n",
       "  97.79882431030273,\n",
       "  97.7314829826355,\n",
       "  97.4158227443695,\n",
       "  97.87458181381226,\n",
       "  97.63888716697693,\n",
       "  97.44949340820312],\n",
       " 'val_accuracy': [98.21969866752625,\n",
       "  98.40909242630005,\n",
       "  97.91666865348816,\n",
       "  97.91666865348816,\n",
       "  98.44697117805481,\n",
       "  98.44697117805481,\n",
       "  98.03030490875244,\n",
       "  97.99242615699768,\n",
       "  97.61363863945007,\n",
       "  97.42424488067627]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "613cfc69",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Lowest] Validation Loss\t\t\t: 5.33%\n",
      "[Highest] Validation Loss\t\t\t: 8.39%\n",
      "Mean Validation Loss\t\t\t\t: 6.62%\n",
      "Standard Deviation of Validation Loss\t\t: 1.00%\n",
      "\n",
      "[Lowest] Validation Accuracy\t\t\t: 97.42%\n",
      "[Highest] Validation Accuracy\t\t\t: 98.45%\n",
      "Mean Validation Accuracy\t\t\t: 98.04%\n",
      "Standard Deviation of Validation Accuracy\t: 0.33%\n",
      "\n",
      "-----------------------------------\n",
      "RESULT OF PREDICTIONS - K-FOLD\n",
      "-----------------------------------\n",
      "+------+------+----------+----------+--------------+\n",
      "| Fold | loss | accuracy | val_loss | val_accuracy |\n",
      "+------+------+----------+----------+--------------+\n",
      "|  6   | 7.57 |  97.73   |   5.93   |    98.45     |\n",
      "|  5   | 7.4  |   97.8   |   5.33   |    98.45     |\n",
      "|  2   | 7.83 |  97.65   |   5.64   |    98.41     |\n",
      "|  1   | 6.6  |  98.16   |   5.98   |    98.22     |\n",
      "|  7   | 8.63 |  97.42   |   6.39   |    98.03     |\n",
      "|  8   | 7.33 |  97.87   |   5.9    |    97.99     |\n",
      "|  4   | 7.44 |  97.77   |   7.24   |    97.92     |\n",
      "|  3   | 7.5  |  97.88   |   7.65   |    97.92     |\n",
      "|  9   | 7.73 |  97.64   |   7.79   |    97.61     |\n",
      "|  10  | 8.39 |  97.45   |   8.39   |    97.42     |\n",
      "+------+------+----------+----------+--------------+\n"
     ]
    }
   ],
   "source": [
    "print('[Lowest] Validation Loss\\t\\t\\t: {:.2f}%'.format(np.min(history['val_loss'])))\n",
    "print('[Highest] Validation Loss\\t\\t\\t: {:.2f}%'.format(np.max(history['val_loss'])))\n",
    "print('Mean Validation Loss\\t\\t\\t\\t: {:.2f}%'.format(mean_loss))\n",
    "print('Standard Deviation of Validation Loss\\t\\t: {:.2f}%'.format(std_loss))\n",
    "print()\n",
    "print('[Lowest] Validation Accuracy\\t\\t\\t: {:.2f}%'.format(np.min(history['val_accuracy'])))\n",
    "print('[Highest] Validation Accuracy\\t\\t\\t: {:.2f}%'.format(np.max(history['val_accuracy'])))\n",
    "print('Mean Validation Accuracy\\t\\t\\t: {:.2f}%'.format(mean_accuracy))\n",
    "print('Standard Deviation of Validation Accuracy\\t: {:.2f}%'.format(std_accuracy))\n",
    "print(\"\\n-----------------------------------\")\n",
    "print(\"RESULT OF PREDICTIONS - K-FOLD\")\n",
    "print(\"-----------------------------------\")\n",
    "from prettytable import PrettyTable\n",
    "  \n",
    "columns = [\"Fold\", \"loss\", \"accuracy\", \"val_loss\", \"val_accuracy\"]\n",
    "  \n",
    "myTable = PrettyTable()\n",
    "  \n",
    "# Add Columns\n",
    "myTable.add_column(columns[0], np.arange(1, n_splits+1))\n",
    "myTable.add_column(columns[1], [str(round(i, 2)) for i in history['loss']])\n",
    "myTable.add_column(columns[2], [str(round(i, 2)) for i in history['accuracy']])\n",
    "myTable.add_column(columns[3], [str(round(i, 2)) for i in history['val_loss']])\n",
    "myTable.add_column(columns[4], [str(round(i, 2)) for i in history['val_accuracy']])\n",
    "\n",
    "# sort the table by salary in descending order\n",
    "myTable.sortby = \"val_accuracy\"\n",
    "myTable.reversesort = True\n",
    "  \n",
    "print(myTable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vFkQEQYVj5sb",
   "metadata": {
    "id": "vFkQEQYVj5sb"
   },
   "source": [
    "# PART 4 : Testing the Loaded Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "xGVuh9xGmzuj",
   "metadata": {
    "id": "xGVuh9xGmzuj"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15960\\2286035052.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mfilename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"{}\\\\{}\\\\{}.h5\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"MODELS\\\\[3-layer] - 3L1\\\\CV\\\\KFold\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"6-f10\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# load model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mloaded_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "filename = \"{}\\\\{}\\\\{}.h5\".format(os.getcwd(), \"MODELS\\\\[3-layer] - 3L1\\\\CV\\\\KFold\", \"6-f10\")\n",
    "\n",
    "# load model\n",
    "loaded_model = load_model(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904cf7c1",
   "metadata": {
    "id": "1xjvuy1flJtr"
   },
   "source": [
    "## Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "be023c88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "825/825 [==============================] - 6s 7ms/step - loss: 0.0589 - accuracy: 0.9826\n",
      "Accuracy \t: 98.26\n",
      "Loss \t\t: 5.89\n"
     ]
    }
   ],
   "source": [
    "score = loaded_model.evaluate(X, hot_y)\n",
    "print(\"Accuracy \\t: {:.2f}\".format(score[1]*100))\n",
    "print(\"Loss \\t\\t: {:.2f}\".format(score[0]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d0c4b8e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "825/825 [==============================] - 6s 7ms/step\n"
     ]
    }
   ],
   "source": [
    "pred = loaded_model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d7336463",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.argmax(hot_y, axis=1)\n",
    "y_pred = np.argmax(pred, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f13e6b",
   "metadata": {},
   "source": [
    "## Plot Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "Om9OAOGfplSe",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "Om9OAOGfplSe",
    "outputId": "3e40fd9c-b99f-43b1-e20f-89472d8ead74"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the confusion matrix\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.imshow(conf_matrix, cmap=plt.cm.Greens)\n",
    "\n",
    "# Add labels to the plot\n",
    "tick_marks = np.arange(len(conf_matrix))\n",
    "plt.xlabel('Predicted label')\n",
    "plt.ylabel('True label')\n",
    "\n",
    "# Add values to the plot\n",
    "for i in range(len(conf_matrix)):\n",
    "    for j in range(len(conf_matrix)):\n",
    "        plt.text(j, i, conf_matrix[i, j], ha='center', va='center')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb04b558",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "yehFnOBDjZcJ"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
