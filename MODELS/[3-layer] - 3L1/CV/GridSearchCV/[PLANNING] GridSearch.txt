GridSearch ---------------- 10-Kfold 

---------------------------------------------------- STANDARD PARAMETERS ---------------------------------------------------------------------------
1. Epoch (15, 20)

general rule of thumb that is often recommended in the deep learning community based on practical experience with different datasets and models.


2. Opimizer (Adam)

Adam is a method for stochastic optimization that combines adaptive learning rate and momentum to effectively optimize large-scale deep neural networks with high-dimensional parameter spaces. The algorithm uses estimates of first and second moments of the gradients to adaptively adjust the learning rate for each parameter, allowing faster convergence and better generalization. Adam also integrates momentum, which reduces the influence of noisy gradient estimates and improves the convergence rate. We show that Adam outperforms other optimization algorithms on a range of deep learning tasks, including language modeling with RNNs.

Reference:
"Adam: A Method for Stochastic Optimization" by Diederik Kingma and Jimmy Ba (2015)



3. batch_size (128, 256)
[not included] batch_size 32 & 64 		 : GPU has the same speed as CPU 

According to the TensorFlow documentation on batching, a batch size of 32 is often a good starting point for RNN LSTM models, but this can vary depending on the specific application and the available hardware resources. It is recommended to experiment with different batch sizes to find the optimal value for a given model and hardware setup.

Reference:
https://www.tensorflow.org/guide/data_performance#batching_dataset_elements

[PERSONAL NOTE]
we try to using batch_size 32 & 64 for CPU and GPU training method, CPU is the winner. Then, we try 128, there a gap about ~9 ms/step with GPU more faster than CPU.
So, we use the batch_size of 128 as pin point, but will also try to use bacth_size of 256, for the sake of experiment.


4. drop-out rate (0.2, 0.3)

In general, dropout rates between 0.2 and 0.5 are common for RNN LSTM models


References:
Gal, Y., & Ghahramani, Z. (2016). Dropout as a Bayesian approximation: Representing model uncertainty in deep learning. In international conference on machine learning (pp. 1050-1059).

Zaremba, W., Sutskever, I., & Vinyals, O. (2014). Recurrent neural network regularization. arXiv preprint arXiv:1409.2329.


[PERSONAL NOTE] we see the previous result is good at 0.2 and we want to experiment if the dropout_rate is 0.3



---------------------------------------------------- HYPER-PARAMETERS ---------------------------------------------------------------------------

1. Network Weight Initialization
['glorot_uniform', 'he_uniform']

Glorot initialization, also known as Xavier initialization, was introduced in the paper "Understanding the difficulty of training deep feedforward neural networks" by Glorot and Bengio (2010). This method scales the weight initialization based on the number of input and output units of each layer, aiming to keep the variance of the activations and gradients roughly the same across different layers.

He initialization, introduced in the paper "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification" by He et al. (2015), is a modification of Glorot initialization that works better for networks that use rectified linear units (ReLU) as activation functions. This method scales the initialization based only on the number of input units, and is designed to avoid the vanishing gradient problem that can occur with deep networks.

Reference:

Glorot, X., & Bengio, Y. (2010). Understanding the difficulty of training deep feedforward neural networks. Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, 9-16.

He, K., Zhang, X., Ren, S., & Sun, J. (2015). Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 1-9.



2. LSTM Activation Function
['tanh']


One common activation function used in LSTM models is the hyperbolic tangent (tanh) function. According to the book "Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow" by Aurélien Géron, "tanh is usually a good choice for recurrent neural networks, including LSTMs, because it provides a stronger gradient than the logistic sigmoid function, which helps reduce the vanishing gradients problem." (page 477)

However, other activation functions such as ReLU may also be suitable for certain tasks and architectures.

Reference:
Géron, A. (2019). Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems (2nd ed.). O'Reilly Media, Inc.


[PERSONAL NOTE]
we can use the ReLu Activation function, but it will not met the criteria of cuDNN (CUDA Deep Neural Network) Library and the effect our training phase will much slower (exacltly 6x times slower). 

More info 			: https://keras.io/api/layers/recurrent_layers/lstm/



3. Recurrent Initializer
['glorot_uniform', 'orthogonal']

INFORMATION:
'zeros': initializes the weights to zero.
'ones': initializes the weights to one.
'uniform': initializes the weights with a uniform distribution.
'glorot_uniform': initializes the weights with the Glorot uniform initializer.
'orthogonal': initializes the weights with an orthogonal matrix.


CHATGPT : Glorot Uniform
DEFAULT : Orthogonal

--------- <Try this, then find the reference for GREATEST RESULT>


4. Neurons
[17, 30, 60]


it is suggested that "the number of neurons in the LSTM layers can be set to the same value as the number of time steps in the input sequence, or to a lower or higher value. A higher number will make the model more expressive and potentially more accurate, but will also make it slower to train and more prone to overfitting."


Reference:
"Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow" by Aurélien Géron


[PERSONAL NOTE]
we taking number of neuron same as the number timestep of our dataset, but will also experimented with other number 30 and 60 (according to previous training, it show 60 is doing well)

---------------------------------------------------- LSTM STRUCTURE ---------------------------------------------------------------------------

LSTM
tf.keras.layers.LSTM(
    units,
    activation="tanh",
    recurrent_activation="sigmoid",
    use_bias=True,
    kernel_initializer="glorot_uniform",
    recurrent_initializer="orthogonal",
    bias_initializer="zeros",
    unit_forget_bias=True,
    kernel_regularizer=None,
    recurrent_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    kernel_constraint=None,
    recurrent_constraint=None,
    bias_constraint=None,
    dropout=0.0,
    recurrent_dropout=0.0,
    return_sequences=False,
    return_state=False,
    go_backwards=False,
    stateful=False,
    time_major=False,
    unroll=False,
    **kwargs
)